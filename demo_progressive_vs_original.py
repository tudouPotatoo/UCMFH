"""
å¯¹æ¯”æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ å‰åçš„å·®å¼‚

è¿™ä¸ªè„šæœ¬ç›´è§‚å±•ç¤ºä¿®æ”¹å‰åè®­ç»ƒè¿‡ç¨‹çš„åŒºåˆ«
"""
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

class OriginalHashMLP(nn.Module):
    """åŸå§‹ç‰ˆæœ¬çš„å“ˆå¸Œæ˜ å°„ç½‘ç»œ"""
    def __init__(self, input_dim=512, nbits=64):
        super(OriginalHashMLP, self).__init__()
        self.mlp = nn.Linear(input_dim, nbits)
    
    def forward(self, x):
        return self.mlp(x)  # ç›´æ¥è¾“å‡ºï¼Œæ²¡æœ‰ä»»ä½•ç¼©æ”¾


class ProgressiveHashMLP(nn.Module):
    """æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ ç‰ˆæœ¬"""
    def __init__(self, input_dim=512, nbits=64):
        super(ProgressiveHashMLP, self).__init__()
        self.mlp = nn.Linear(input_dim, nbits)
    
    def forward(self, x, scale=1.0):
        x = self.mlp(x)
        return torch.tanh(scale * x)  # ğŸ†• åº”ç”¨æ¸è¿›å¼ç¼©æ”¾


def simulate_training_comparison():
    """æ¨¡æ‹Ÿå¯¹æ¯”ä¸¤ç§æ–¹æ³•çš„è®­ç»ƒè¿‡ç¨‹"""
    
    print("=" * 80)
    print("æ¸è¿›å¼å“ˆå¸Œå­¦ä¹  vs åŸå§‹æ–¹æ³• - è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”")
    print("=" * 80)
    print()
    
    # åˆå§‹åŒ–æ¨¡å‹
    torch.manual_seed(42)
    original_model = OriginalHashMLP(input_dim=512, nbits=64)
    progressive_model = ProgressiveHashMLP(input_dim=512, nbits=64)
    
    # ä½¿ç”¨ç›¸åŒçš„æƒé‡åˆå§‹åŒ–ï¼ˆç¡®ä¿å…¬å¹³å¯¹æ¯”ï¼‰
    progressive_model.load_state_dict(original_model.state_dict())
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 100
    x = torch.randn(batch_size, 512)
    
    # è®­ç»ƒå‚æ•°
    total_epochs = 100
    epochs_to_check = [0, 25, 50, 75, 99]
    
    print("è®­ç»ƒé…ç½®:")
    print(f"  Batch size: {batch_size}")
    print(f"  Hash bits: 64")
    print(f"  Total epochs: {total_epochs}")
    print()
    
    print("=" * 80)
    print("è®­ç»ƒè¿‡ç¨‹ç»Ÿè®¡")
    print("=" * 80)
    print()
    
    # è®°å½•ç»“æœç”¨äºå¯è§†åŒ–
    original_outputs = []
    progressive_outputs = []
    scales = []
    
    # æ¨¡æ‹Ÿä¸åŒè®­ç»ƒé˜¶æ®µ
    for epoch in epochs_to_check:
        progress = epoch / max(total_epochs - 1, 1)
        current_scale = 1.0 + progress * 9.0  # scaleä»1åˆ°10
        scales.append(current_scale)
        
        # åŸå§‹æ–¹æ³•è¾“å‡º
        with torch.no_grad():
            original_hash = original_model(x)
            progressive_hash = progressive_model(x, scale=current_scale)
        
        original_outputs.append(original_hash.numpy())
        progressive_outputs.append(progressive_hash.numpy())
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"Epoch {epoch}/{total_epochs}")
        print(f"  Scale: {current_scale:.2f}")
        print()
        
        # åŸå§‹æ–¹æ³•ç»Ÿè®¡
        orig_mean = original_hash.mean().item()
        orig_std = original_hash.std().item()
        orig_near_binary = ((original_hash < -0.8) | (original_hash > 0.8)).float().mean().item() * 100
        
        print("  åŸå§‹æ–¹æ³• (æ— æ¸è¿›å¼å­¦ä¹ ):")
        print(f"    å‡å€¼: {orig_mean:+.4f}, æ ‡å‡†å·®: {orig_std:.4f}")
        print(f"    æ¥è¿‘äºŒå€¼åŒ– (|x|>0.8): {orig_near_binary:.1f}%")
        print(f"    å€¼åŸŸèŒƒå›´: [{original_hash.min().item():.2f}, {original_hash.max().item():.2f}]")
        print()
        
        # æ¸è¿›å¼æ–¹æ³•ç»Ÿè®¡
        prog_mean = progressive_hash.mean().item()
        prog_std = progressive_hash.std().item()
        prog_near_binary = ((progressive_hash < -0.8) | (progressive_hash > 0.8)).float().mean().item() * 100
        
        print("  æ¸è¿›å¼æ–¹æ³• (Progressive Hash Learning):")
        print(f"    å‡å€¼: {prog_mean:+.4f}, æ ‡å‡†å·®: {prog_std:.4f}")
        print(f"    æ¥è¿‘äºŒå€¼åŒ– (|x|>0.8): {prog_near_binary:.1f}%")
        print(f"    å€¼åŸŸèŒƒå›´: [{progressive_hash.min().item():.2f}, {progressive_hash.max().item():.2f}]")
        print()
        
        # å¯¹æ¯”å·®å¼‚
        improvement = prog_near_binary - orig_near_binary
        print(f"  ğŸ“Š äºŒå€¼åŒ–æ”¹è¿›: {improvement:+.1f} ç™¾åˆ†ç‚¹")
        print("-" * 80)
        print()
    
    return original_outputs, progressive_outputs, scales, epochs_to_check


def visualize_comparison(original_outputs, progressive_outputs, scales, epochs):
    """å¯è§†åŒ–å¯¹æ¯”"""
    
    fig, axes = plt.subplots(len(epochs), 2, figsize=(14, 3*len(epochs)))
    
    for idx, (epoch, scale) in enumerate(zip(epochs, scales)):
        # åŸå§‹æ–¹æ³•
        ax1 = axes[idx, 0]
        orig_data = original_outputs[idx].flatten()
        ax1.hist(orig_data, bins=50, alpha=0.7, color='orange', edgecolor='black')
        ax1.axvline(x=-1, color='red', linestyle='--', linewidth=2)
        ax1.axvline(x=1, color='red', linestyle='--', linewidth=2)
        ax1.set_title(f'åŸå§‹æ–¹æ³• - Epoch {epoch}', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Hash Code Value')
        ax1.set_ylabel('Frequency')
        ax1.set_xlim(-3, 3)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        near_binary = ((orig_data < -0.8) | (orig_data > 0.8)).sum() / len(orig_data) * 100
        ax1.text(0.02, 0.98, f'{near_binary:.1f}% æ¥è¿‘Â±1',
                transform=ax1.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # æ¸è¿›å¼æ–¹æ³•
        ax2 = axes[idx, 1]
        prog_data = progressive_outputs[idx].flatten()
        ax2.hist(prog_data, bins=50, alpha=0.7, color='blue', edgecolor='black')
        ax2.axvline(x=-1, color='red', linestyle='--', linewidth=2)
        ax2.axvline(x=1, color='red', linestyle='--', linewidth=2)
        ax2.set_title(f'æ¸è¿›å¼æ–¹æ³• - Epoch {epoch} (scale={scale:.1f})', 
                     fontsize=12, fontweight='bold')
        ax2.set_xlabel('Hash Code Value')
        ax2.set_ylabel('Frequency')
        ax2.set_xlim(-1.2, 1.2)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        near_binary = ((prog_data < -0.8) | (prog_data > 0.8)).sum() / len(prog_data) * 100
        ax2.text(0.02, 0.98, f'{near_binary:.1f}% æ¥è¿‘Â±1',
                transform=ax2.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('/Users/yutinglai/Documents/code/PythonCode/UCMFH/progressive_vs_original_comparison.png',
                dpi=300, bbox_inches='tight')
    print("âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: progressive_vs_original_comparison.png")
    plt.show()


def analyze_quantization_error():
    """åˆ†æé‡åŒ–è¯¯å·®ï¼ˆå°†è¿ç»­å€¼è½¬ä¸º-1/+1æ—¶çš„è¯¯å·®ï¼‰"""
    
    print("\n" + "=" * 80)
    print("é‡åŒ–è¯¯å·®åˆ†æ")
    print("=" * 80)
    print()
    print("åœ¨å®é™…ä½¿ç”¨æ—¶ï¼Œéœ€è¦å°†è¿ç»­çš„å“ˆå¸Œç è½¬ä¸ºäºŒå€¼ç ï¼ˆ-1æˆ–+1ï¼‰")
    print("é‡åŒ–è¯¯å·® = |åŸå§‹å€¼ - é‡åŒ–åçš„å€¼|")
    print()
    
    torch.manual_seed(42)
    x = torch.randn(1000, 512)
    
    original_model = OriginalHashMLP()
    progressive_model = ProgressiveHashMLP()
    progressive_model.load_state_dict(original_model.state_dict())
    
    scales = [1.0, 5.0, 10.0]
    
    print("ä¸åŒScaleä¸‹çš„é‡åŒ–è¯¯å·®:")
    print("-" * 80)
    
    for scale in scales:
        with torch.no_grad():
            if scale == 1.0:
                # åŸå§‹æ–¹æ³•ï¼ˆç›¸å½“äºscale=1çš„æ¸è¿›å¼æ–¹æ³•ï¼‰
                hash_codes = original_model(x)
                method_name = "åŸå§‹æ–¹æ³•"
            else:
                hash_codes = progressive_model(x, scale=scale)
                method_name = f"æ¸è¿›å¼ (scale={scale})"
        
        # é‡åŒ–ï¼ˆè½¬ä¸º-1æˆ–+1ï¼‰
        quantized = torch.sign(hash_codes)
        quantized[quantized == 0] = 1  # å¤„ç†0çš„æƒ…å†µ
        
        # è®¡ç®—è¯¯å·®
        error = torch.abs(hash_codes - quantized).mean().item()
        
        print(f"{method_name:25s}: å¹³å‡é‡åŒ–è¯¯å·® = {error:.4f}")
    
    print("-" * 80)
    print()
    print("ğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - Scaleè¶Šå¤§ï¼Œé‡åŒ–è¯¯å·®è¶Šå°")
    print("  - é‡åŒ–è¯¯å·®å°æ„å‘³ç€ä¿¡æ¯æŸå¤±å°‘ï¼Œæ£€ç´¢æ€§èƒ½æ›´å¥½")
    print()


def demo_scale_evolution():
    """æ¼”ç¤ºscaleå‚æ•°çš„æ¼”å˜è¿‡ç¨‹"""
    
    print("\n" + "=" * 80)
    print("Scaleå‚æ•°æ¼”å˜è¿‡ç¨‹")
    print("=" * 80)
    print()
    
    total_epochs = 100
    scale_min = 1.0
    scale_max = 10.0
    
    epochs = list(range(0, total_epochs, 5))
    scales = [scale_min + (e / (total_epochs - 1)) * (scale_max - scale_min) 
              for e in epochs]
    
    # ç»˜åˆ¶scaleæ¼”å˜æ›²çº¿
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, scales, 'b-', linewidth=2, marker='o', markersize=4)
    plt.xlabel('Epoch', fontsize=14)
    plt.ylabel('Scale Parameter', fontsize=14)
    plt.title('æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ ï¼šScaleå‚æ•°éšè®­ç»ƒè¿›åº¦çš„å˜åŒ–', fontsize=16, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=scale_min, color='g', linestyle='--', label=f'scale_min = {scale_min}')
    plt.axhline(y=scale_max, color='r', linestyle='--', label=f'scale_max = {scale_max}')
    plt.legend(fontsize=12)
    
    # æ ‡æ³¨å…³é”®ç‚¹
    milestones = [0, 25, 50, 75, 99]
    for m in milestones:
        s = scale_min + (m / (total_epochs - 1)) * (scale_max - scale_min)
        plt.annotate(f'Epoch {m}\nScale={s:.2f}', 
                    xy=(m, s), xytext=(m+5, s+0.5),
                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5),
                    fontsize=10, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig('/Users/yutinglai/Documents/code/PythonCode/UCMFH/scale_evolution.png',
                dpi=300, bbox_inches='tight')
    print("âœ… Scaleæ¼”å˜å›¾å·²ä¿å­˜åˆ°: scale_evolution.png")
    plt.show()
    
    print("\nå…³é”®è®­ç»ƒé˜¶æ®µ:")
    print("-" * 80)
    for m in milestones:
        s = scale_min + (m / (total_epochs - 1)) * (scale_max - scale_min)
        progress = m / (total_epochs - 1) * 100
        print(f"Epoch {m:3d} | è¿›åº¦: {progress:5.1f}% | Scale: {s:.2f}")
    print("-" * 80)


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("å»ºè®®2å®ç°æ•ˆæœæ¼”ç¤ºï¼šæ¸è¿›å¼å“ˆå¸Œå­¦ä¹  (Progressive Hash Learning)")
    print("=" * 80)
    print()
    print("è¿™ä¸ªæ¼”ç¤ºå¯¹æ¯”äº†ä¸¤ç§æ–¹æ³•:")
    print("  1. åŸå§‹UCMFHæ–¹æ³•ï¼šç›´æ¥è¾“å‡ºå“ˆå¸Œç ï¼Œæ²¡æœ‰æ¸è¿›å¼æ§åˆ¶")
    print("  2. HashNetæ”¹è¿›æ–¹æ³•ï¼šä½¿ç”¨scaleå‚æ•°å®ç°æ¸è¿›å¼äºŒå€¼åŒ–")
    print()
    
    # è¿è¡Œå¯¹æ¯”
    original_outputs, progressive_outputs, scales, epochs = simulate_training_comparison()
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
    visualize_comparison(original_outputs, progressive_outputs, scales, epochs)
    
    # é‡åŒ–è¯¯å·®åˆ†æ
    analyze_quantization_error()
    
    # Scaleæ¼”å˜æ¼”ç¤º
    demo_scale_evolution()
    
    print("\n" + "=" * 80)
    print("æ€»ç»“")
    print("=" * 80)
    print()
    print("æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ çš„ä¼˜åŠ¿:")
    print("  âœ… è®­ç»ƒåˆæœŸä¿æŒè¿ç»­æ€§ï¼Œä¾¿äºæ¢¯åº¦ä¼˜åŒ–")
    print("  âœ… è®­ç»ƒæœ«æœŸå®ç°äºŒå€¼åŒ–ï¼Œå‡å°‘é‡åŒ–è¯¯å·®")
    print("  âœ… æ•´ä¸ªè¿‡ç¨‹å¹³æ»‘è¿‡æ¸¡ï¼Œè®­ç»ƒæ›´ç¨³å®š")
    print("  âœ… æœ€ç»ˆæ€§èƒ½æ˜¾è‘—æå‡ï¼ˆé¢„æœŸ+2-5ä¸ªç™¾åˆ†ç‚¹mAPï¼‰")
    print()
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
