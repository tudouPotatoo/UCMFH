import torch
import torch.nn as nn
from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm, Dropout, Linear
from torch.nn.functional import normalize
from torch.nn import functional as F

class UnimodalTransformer(nn.Module):
    """å•æ¨¡æ€Transformerç¼–ç å™¨ï¼Œç”¨äºŽå­¦ä¹ å›¾åƒæˆ–æ–‡æœ¬çš„å†…éƒ¨ç‰¹å¾å…³ç³»"""
    def __init__(self, d_model=512, nhead=8, num_layers=2, dim_feedforward=2048, dropout=0.1):
        super(UnimodalTransformer, self).__init__()
        encoder_layer = TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.norm = LayerNorm(d_model)
        
    def forward(self, x):
        # x: [batch_size, d_model]
        x = x.unsqueeze(1)  # [batch_size, 1, d_model] - æ·»åŠ åºåˆ—ç»´åº¦
        x = self.transformer(x)  # Transformerç¼–ç 
        x = x.squeeze(1)  # [batch_size, d_model] - ç§»é™¤åºåˆ—ç»´åº¦
        return self.norm(x)


class CrossAttentionFusion(nn.Module):
    """åŒå‘Cross-Attentionèžåˆæ¨¡å—"""
    def __init__(self, d_model=512, nhead=8, dropout=0.1):
        super(CrossAttentionFusion, self).__init__()
        
        # Image-to-Text Cross-Attention (å›¾åƒä½œä¸ºQueryï¼Œä»Žæ–‡æœ¬ä¸­èŽ·å–ä¿¡æ¯)
        self.img2text_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            batch_first=True
        )
        
        # Text-to-Image Cross-Attention (æ–‡æœ¬ä½œä¸ºQueryï¼Œä»Žå›¾åƒä¸­èŽ·å–ä¿¡æ¯)
        self.text2img_attn = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Normalization (ç”¨äºŽæ®‹å·®è¿žæŽ¥åŽçš„å½’ä¸€åŒ–)
        self.norm_img = LayerNorm(d_model)
        self.norm_text = LayerNorm(d_model)
        
        # Dropout for residual connection
        self.dropout = Dropout(dropout)
        
    def forward(self, img_feat, text_feat):
        """
        åŒå‘Cross-Attentionäº¤äº’
        Args:
            img_feat: [batch_size, d_model] å›¾åƒç‰¹å¾
            text_feat: [batch_size, d_model] æ–‡æœ¬ç‰¹å¾
        Returns:
            img_fused: [batch_size, d_model] èžåˆåŽçš„å›¾åƒç‰¹å¾
            text_fused: [batch_size, d_model] èžåˆåŽçš„æ–‡æœ¬ç‰¹å¾
        """
        # æ·»åŠ åºåˆ—ç»´åº¦ä»¥é€‚é…MultiheadAttention
        img = img_feat.unsqueeze(1)   # [batch, 1, d_model]
        text = text_feat.unsqueeze(1) # [batch, 1, d_model]
        
        # ç¬¬ä¸€ä¸ªæ–¹å‘ï¼šå›¾åƒä½œä¸ºQueryï¼Œæ–‡æœ¬ä½œä¸ºKey/Value
        # è®©å›¾åƒç‰¹å¾ä»Žæ–‡æœ¬ä¸­èŽ·å–äº’è¡¥ä¿¡æ¯
        img_attended, _ = self.img2text_attn(
            query=img,
            key=text,
            value=text
        )  # [batch, 1, d_model]
        # æ®‹å·®è¿žæŽ¥ + LayerNorm
        img_fused = self.norm_img(img + self.dropout(img_attended))
        
        # ç¬¬äºŒä¸ªæ–¹å‘ï¼šæ–‡æœ¬ä½œä¸ºQueryï¼Œå›¾åƒä½œä¸ºKey/Value
        # è®©æ–‡æœ¬ç‰¹å¾ä»Žå›¾åƒä¸­èŽ·å–äº’è¡¥ä¿¡æ¯
        text_attended, _ = self.text2img_attn(
            query=text,
            key=img,
            value=img
        )  # [batch, 1, d_model]
        # æ®‹å·®è¿žæŽ¥ + LayerNorm
        text_fused = self.norm_text(text + self.dropout(text_attended))
        
        # ç§»é™¤åºåˆ—ç»´åº¦
        img_fused = img_fused.squeeze(1)   # [batch, d_model]
        text_fused = text_fused.squeeze(1) # [batch, d_model]
        
        return img_fused, text_fused


class FuseTransEncoder(nn.Module):
    """æ”¹è¿›çš„åŒæµ+Cross-Attentionç¼–ç å™¨ (ç§»é™¤å†—ä½™çš„TransformerEncoder)"""
    def __init__(self,  num_layers, hidden_size, nhead):
        super(FuseTransEncoder, self).__init__()
        # ç¬¬ä¸€é˜¶æ®µï¼šåŒæµæž¶æž„ å•æ¨¡æ€Transformer (å­¦ä¹ å›¾åƒ/æ–‡æœ¬å†…éƒ¨å…³ç³»)
        self.image_transformer = UnimodalTransformer(d_model=512, nhead=8, num_layers=2, dropout=0.1)
        self.text_transformer = UnimodalTransformer(d_model=512, nhead=8, num_layers=2, dropout=0.1)
        
        # ç¬¬äºŒé˜¶æ®µï¼šåŒå‘Cross-Attention (æ›¿ä»£åŽŸæ¥çš„å†—ä½™TransformerEncoder)
        self.cross_attention = CrossAttentionFusion(d_model=512, nhead=8, dropout=0.1)
        
        self.d_model = hidden_size
        self.sigal_d = int(self.d_model/2)
       
    def forward(self, tokens):
        """
        å‰å‘ä¼ æ’­ï¼šå•æ¨¡æ€å¢žå¼º -> åŒå‘Cross-Attentionèžåˆ
        Args:
            tokens: [1, batch_size, 1024] æˆ– [batch_size, 1024]
        Returns:
            img: [batch_size, 512] èžåˆåŽçš„å›¾åƒç‰¹å¾
            txt: [batch_size, 512] èžåˆåŽçš„æ–‡æœ¬ç‰¹å¾
        """
        # å¤„ç†è¾“å…¥ç»´åº¦ (å…¼å®¹åŽŸæœ‰ä»£ç çš„ä¸åŒè¾“å…¥æ ¼å¼)
        if tokens.dim() == 3:
            tokens_reshaped = tokens.reshape(-1, self.d_model)  # [batch_size, 1024]
        else:
            tokens_reshaped = tokens  # [batch_size, 1024]
        
        # åˆ†ç¦»å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
        img_input = tokens_reshaped[:, :self.sigal_d]  # [batch_size, 512]
        txt_input = tokens_reshaped[:, self.sigal_d:]  # [batch_size, 512]
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå•æ¨¡æ€ç‰¹å¾å¢žå¼º
        img_enhanced = self.image_transformer(img_input)  # [batch_size, 512]
        txt_enhanced = self.text_transformer(txt_input)  # [batch_size, 512]
        
        # ç¬¬äºŒé˜¶æ®µï¼šåŒå‘Cross-Attentionèžåˆ (æ›¿ä»£åŽŸæ¥çš„è·¨æ¨¡æ€èžåˆTransformer)
        # å›¾åƒä»Žæ–‡æœ¬èŽ·å–ä¿¡æ¯ï¼Œæ–‡æœ¬ä»Žå›¾åƒèŽ·å–ä¿¡æ¯
        img_fused, txt_fused = self.cross_attention(img_enhanced, txt_enhanced)
        
        # L2å½’ä¸€åŒ– (ä¿æŒä¸ŽåŽŸä»£ç ä¸€è‡´)
        img = normalize(img_fused, p=2, dim=1)  # [batch_size, 512]
        txt = normalize(txt_fused, p=2, dim=1)  # [batch_size, 512]
        
        return img, txt


class ImageMlp(nn.Module):
    """
    å›¾åƒå“ˆå¸Œæ˜ å°„ç½‘ç»œ (å¸¦æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ )
    
    å€Ÿé‰´HashNetçš„"Deep Learning to Hash by Continuation"æ€æƒ³ï¼š
    - è®­ç»ƒåˆæœŸï¼šä½¿ç”¨è¾ƒå°çš„scaleï¼Œè¾“å‡ºè¿žç»­å€¼ï¼ˆæ˜“äºŽä¼˜åŒ–ï¼‰
    - è®­ç»ƒä¸­æœŸï¼šé€æ¸å¢žå¤§scaleï¼Œè¾“å‡ºå‘-1/+1é æ‹¢
    - è®­ç»ƒåŽæœŸï¼šscaleå¾ˆå¤§ï¼Œè¾“å‡ºå‡ ä¹Žæ˜¯äºŒå€¼åŒ–çš„
    
    è¿™æ ·å¯ä»¥ï¼š
    1. é¿å…ç›´æŽ¥å­¦ä¹ ç¦»æ•£å€¼çš„å›°éš¾
    2. ä¿æŒæ¢¯åº¦æµç•…ï¼Œè®­ç»ƒæ›´ç¨³å®š
    3. æœ€ç»ˆèŽ·å¾—æ›´å¥½çš„äºŒå€¼å“ˆå¸Œç 
    """
    def __init__(self, input_dim=512, hash_lens=64, use_progressive=True):
        super(ImageMlp, self).__init__()
        self.fc1 = nn.Linear(input_dim, 1024)
        self.fc2 = nn.Linear(1024, hash_lens)
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.3)
        self.use_progressive = use_progressive
        
    def forward(self, x, scale=1.0):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, input_dim]
            scale: ç¼©æ”¾å‚æ•°ï¼ŒæŽ§åˆ¶è¾“å‡ºçš„"é”åº¦"
                  - scale=1: è¾“å‡ºè¾ƒ"è½¯"ï¼ŒèŒƒå›´å¹¿
                  - scale=10: è¾“å‡ºå¾ˆ"ç¡¬"ï¼ŒæŽ¥è¿‘-1æˆ–+1
        Returns:
            å“ˆå¸Œç è¡¨ç¤º [batch_size, hash_lens]
        """
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # [batch_size, hash_lens]
        
        # ðŸ†• æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ 
        if self.use_progressive and scale > 1.0:
            # ä½¿ç”¨tanhå°†è¾“å‡ºåŽ‹ç¼©åˆ°(-1, 1)ï¼ŒscaleæŽ§åˆ¶é™¡å³­ç¨‹åº¦
            # scaleè¶Šå¤§ï¼Œè¾“å‡ºè¶ŠæŽ¥è¿‘-1æˆ–+1
            x = torch.tanh(scale * x)
        
        return normalize(x, p=2, dim=1)

class TextMlp(nn.Module):
    """
    æ–‡æœ¬å“ˆå¸Œæ˜ å°„ç½‘ç»œ (å¸¦æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ )
    
    åŒImageMlpï¼Œæ”¯æŒæ¸è¿›å¼å“ˆå¸Œç å­¦ä¹ 
    """
    def __init__(self, input_dim=512, hash_lens=64, use_progressive=True):
        super(TextMlp, self).__init__()
        self.fc1 = nn.Linear(input_dim, 1024)
        self.fc2 = nn.Linear(1024, hash_lens)
        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.3)
        self.use_progressive = use_progressive
        
    def forward(self, x, scale=1.0):
        """
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, input_dim]
            scale: ç¼©æ”¾å‚æ•°ï¼ŒæŽ§åˆ¶è¾“å‡ºçš„"é”åº¦"
        Returns:
            å“ˆå¸Œç è¡¨ç¤º [batch_size, hash_lens]
        """
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # [batch_size, hash_lens]
        
        # ðŸ†• æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ 
        if self.use_progressive and scale > 1.0:
            x = torch.tanh(scale * x)
        
        return normalize(x, p=2, dim=1)


