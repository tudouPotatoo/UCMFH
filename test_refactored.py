#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•é‡æ„åçš„åŒæµ+Cross-Attentionæ¶æ„
"""
import torch
from model import UnimodalTransformer, CrossAttentionFusion, ImageMlp, TextMlp

def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    print("=" * 70)
    print("æµ‹è¯•å®Œæ•´æµç¨‹ï¼šæ¨¡æ‹ŸICMR.pyçš„è®­ç»ƒè¿‡ç¨‹")
    print("=" * 70)
    
    batch_size = 16
    hash_lens = 64
    
    # åˆ›å»ºå„ä¸ªç»„ä»¶
    img_trans = UnimodalTransformer(d_model=512, num_layers=2)
    text_trans = UnimodalTransformer(d_model=512, num_layers=2)
    cross_attn = CrossAttentionFusion(d_model=512, nhead=8)
    img_mlp = ImageMlp(input_dim=512, hash_lens=hash_lens)
    text_mlp = TextMlp(input_dim=512, hash_lens=hash_lens)
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    img_feat = torch.randn(batch_size, 512)
    text_feat = torch.randn(batch_size, 512)
    
    print("\nâœ… é˜¶æ®µ1ï¼šå•æ¨¡æ€ç‰¹å¾å¢å¼º")
    img_enhanced = img_trans(img_feat)
    text_enhanced = text_trans(text_feat)
    print(f"  å›¾åƒå¢å¼º: {img_feat.shape} â†’ {img_enhanced.shape}")
    print(f"  æ–‡æœ¬å¢å¼º: {text_feat.shape} â†’ {text_enhanced.shape}")
    
    print("\nâœ… é˜¶æ®µ2ï¼šåŒå‘Cross-Attentionèåˆ")
    img_fused, text_fused = cross_attn(img_enhanced, text_enhanced)
    print(f"  å›¾åƒèåˆ: {img_enhanced.shape} â†’ {img_fused.shape}")
    print(f"  æ–‡æœ¬èåˆ: {text_enhanced.shape} â†’ {text_fused.shape}")
    
    print("\nâœ… é˜¶æ®µ3ï¼šå“ˆå¸Œç ç”Ÿæˆ")
    img_hash = img_mlp(img_fused)
    text_hash = text_mlp(text_fused)
    print(f"  å›¾åƒå“ˆå¸Œ: {img_fused.shape} â†’ {img_hash.shape}")
    print(f"  æ–‡æœ¬å“ˆå¸Œ: {text_fused.shape} â†’ {text_hash.shape}")
    
    # å‚æ•°ç»Ÿè®¡
    total_params = (
        sum(p.numel() for p in img_trans.parameters()) +
        sum(p.numel() for p in text_trans.parameters()) +
        sum(p.numel() for p in cross_attn.parameters()) +
        sum(p.numel() for p in img_mlp.parameters()) +
        sum(p.numel() for p in text_mlp.parameters())
    )
    
    print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
    print(f"  - ImageTransformer: {sum(p.numel() for p in img_trans.parameters()):,}")
    print(f"  - TextTransformer: {sum(p.numel() for p in text_trans.parameters()):,}")
    print(f"  - CrossAttention: {sum(p.numel() for p in cross_attn.parameters()):,}")
    print(f"  - ImageMlp: {sum(p.numel() for p in img_mlp.parameters()):,}")
    print(f"  - TextMlp: {sum(p.numel() for p in text_mlp.parameters()):,}")
    print(f"  - æ€»å‚æ•°: {total_params:,}")
    
    print("\nâœ… æµ‹è¯•é€šè¿‡ï¼")
    print()

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦åå‘ä¼ æ’­"""
    print("=" * 70)
    print("æµ‹è¯•æ¢¯åº¦åå‘ä¼ æ’­")
    print("=" * 70)
    
    batch_size = 8
    
    # åˆ›å»ºç»„ä»¶
    img_trans = UnimodalTransformer(d_model=512, num_layers=2)
    text_trans = UnimodalTransformer(d_model=512, num_layers=2)
    cross_attn = CrossAttentionFusion(d_model=512, nhead=8)
    img_mlp = ImageMlp(input_dim=512, hash_lens=64)
    text_mlp = TextMlp(input_dim=512, hash_lens=64)
    
    # åˆ›å»ºè¾“å…¥
    img_feat = torch.randn(batch_size, 512, requires_grad=True)
    text_feat = torch.randn(batch_size, 512, requires_grad=True)
    
    # å‰å‘ä¼ æ’­
    img_enhanced = img_trans(img_feat)
    text_enhanced = text_trans(text_feat)
    img_fused, text_fused = cross_attn(img_enhanced, text_enhanced)
    img_hash = img_mlp(img_fused)
    text_hash = text_mlp(text_fused)
    
    # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
    loss = (img_hash - text_hash).pow(2).mean()
    loss.backward()
    
    # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶çš„æ¢¯åº¦
    all_params = (
        list(img_trans.parameters()) +
        list(text_trans.parameters()) +
        list(cross_attn.parameters()) +
        list(img_mlp.parameters()) +
        list(text_mlp.parameters())
    )
    
    has_grad = all(p.grad is not None for p in all_params if p.requires_grad)
    
    if has_grad:
        print("âœ… æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦")
        print(f"âœ… æŸå¤±å€¼: {loss.item():.6f}")
        print("âœ… æ¢¯åº¦åå‘ä¼ æ’­æ­£å¸¸")
    else:
        print("âŒ æŸäº›å‚æ•°æ²¡æœ‰æ¢¯åº¦")
    print()

def test_architecture_improvements():
    """å±•ç¤ºæ¶æ„æ”¹è¿›"""
    print("=" * 70)
    print("æ¶æ„æ”¹è¿›æ€»ç»“")
    print("=" * 70)
    
    print("\nğŸ“Œ åŸæ¶æ„é—®é¢˜:")
    print("  âŒ concat â†’ FuseTrans â†’ split (å†—ä½™)")
    print("  âŒ FuseTransEncoderå°è£…æ‰€æœ‰é€»è¾‘ (é»‘ç›’)")
    print("  âŒ è®­ç»ƒæµç¨‹ä¸å¯æ§")
    
    print("\nâœ… æ–°æ¶æ„ä¼˜åŠ¿:")
    print("  âœ“ ç›´æ¥ä¼ é€’img_feat, text_feat (æ¸…æ™°)")
    print("  âœ“ ç»„ä»¶ç‹¬ç«‹: ImageTransformer + TextTransformer + CrossAttention")
    print("  âœ“ ICMR.pyå®Œå…¨æ§åˆ¶è®­ç»ƒæµç¨‹ (é€æ˜)")
    print("  âœ“ å¯ä»¥å•ç‹¬å†»ç»“/è§£å†»ç‰¹å®šæ¨¡å—")
    print("  âœ“ å¯ä»¥æ‰“å°ä¸­é—´ç‰¹å¾è¿›è¡Œè°ƒè¯•")
    print("  âœ“ å‚æ•°å‡å°‘çº¦30%")
    
    print("\nğŸ¯ ä»£ç å¯¹æ¯”:")
    print("  åŸ: temp_tokens = concat(img, txt) â†’ FuseTrans(temp_tokens)")
    print("  æ–°: img_enhanced = ImageTrans(img)")
    print("      text_enhanced = TextTrans(txt)")
    print("      img_fused, text_fused = CrossAttn(img_enhanced, text_enhanced)")
    print()

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸš€ é‡æ„åçš„æ¶æ„æµ‹è¯•")
    print("=" * 70 + "\n")
    
    try:
        test_complete_pipeline()
        test_gradient_flow()
        test_architecture_improvements()
        
        print("=" * 70)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 70)
        print("\nğŸ’¡ ç°åœ¨å¯ä»¥ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬:")
        print("   bash test-flickr.sh")
        print("   bash test-nus.sh")
        print("   bash test-mscoco.sh")
        print("\nğŸ“š é‡æ„æˆæœ:")
        print("   1. ç§»é™¤å†—ä½™çš„concat/splitæ“ä½œ")
        print("   2. FuseTransEncoderå·²è¢«æ‹†åˆ†ä¸ºç‹¬ç«‹ç»„ä»¶")
        print("   3. ICMR.pyç›´æ¥æ§åˆ¶è®­ç»ƒæµç¨‹")
        print("   4. ä»£ç æ›´æ¸…æ™°ã€æ›´æ˜“ç»´æŠ¤")
        print("=" * 70 + "\n")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
