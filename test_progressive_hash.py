"""
æµ‹è¯•æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ çš„æ•ˆæœ

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†éšç€è®­ç»ƒè¿›åº¦å¢åŠ ï¼Œscaleå‚æ•°å¦‚ä½•å½±å“å“ˆå¸Œç çš„åˆ†å¸ƒ
"""
import torch
import numpy as np
import matplotlib.pyplot as plt

def visualize_progressive_hash():
    """å¯è§†åŒ–æ¸è¿›å¼å“ˆå¸Œå­¦ä¹ è¿‡ç¨‹"""
    
    # æ¨¡æ‹Ÿä¸€æ‰¹å“ˆå¸Œç åŸå§‹å€¼ï¼ˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿ç»­å€¼ï¼‰
    torch.manual_seed(42)
    x = torch.randn(100) * 0.5  # 100ä¸ªæ ·æœ¬ï¼ŒèŒƒå›´å¤§è‡´åœ¨[-1, 1]ä¹‹é—´
    
    # ä¸åŒè®­ç»ƒé˜¶æ®µçš„scaleå€¼
    scales = [1.0, 2.0, 4.0, 6.0, 8.0, 10.0]
    epochs = [0, 20, 40, 60, 80, 100]  # å‡è®¾æ€»å…±100ä¸ªepoch
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, (scale, epoch) in enumerate(zip(scales, epochs)):
        # åº”ç”¨æ¸è¿›å¼å˜æ¢
        y = torch.tanh(scale * x)
        
        # ç»˜åˆ¶ç›´æ–¹å›¾
        ax = axes[idx]
        ax.hist(y.numpy(), bins=50, alpha=0.7, color='blue', edgecolor='black')
        ax.axvline(x=-1, color='red', linestyle='--', linewidth=2, label='ç›®æ ‡å€¼ -1')
        ax.axvline(x=1, color='red', linestyle='--', linewidth=2, label='ç›®æ ‡å€¼ +1')
        ax.set_title(f'Epoch {epoch} (scale={scale})', fontsize=14, fontweight='bold')
        ax.set_xlabel('Hash Code Value', fontsize=12)
        ax.set_ylabel('Frequency', fontsize=12)
        ax.set_xlim(-1.2, 1.2)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        close_to_binary = ((y < -0.8) | (y > 0.8)).sum().item() / len(y) * 100
        ax.text(0.02, 0.98, f'{close_to_binary:.1f}% æ¥è¿‘äºŒå€¼åŒ–',
                transform=ax.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig('/Users/yutinglai/Documents/code/PythonCode/UCMFH/progressive_hash_visualization.png', 
                dpi=300, bbox_inches='tight')
    print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: progressive_hash_visualization.png")
    plt.show()


def compare_hash_distributions():
    """å¯¹æ¯”ä¸åŒscaleä¸‹çš„å“ˆå¸Œç åˆ†å¸ƒ"""
    
    torch.manual_seed(42)
    x = torch.randn(1000) * 0.5
    
    scales = [1.0, 5.0, 10.0]
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    
    for scale in scales:
        y = torch.tanh(scale * x)
        ax.hist(y.numpy(), bins=50, alpha=0.5, label=f'scale={scale}', edgecolor='black')
    
    ax.axvline(x=-1, color='red', linestyle='--', linewidth=2)
    ax.axvline(x=1, color='red', linestyle='--', linewidth=2)
    ax.set_xlabel('Hash Code Value', fontsize=14)
    ax.set_ylabel('Frequency', fontsize=14)
    ax.set_title('ä¸åŒScaleå‚æ•°ä¸‹çš„å“ˆå¸Œç åˆ†å¸ƒå¯¹æ¯”', fontsize=16, fontweight='bold')
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('/Users/yutinglai/Documents/code/PythonCode/UCMFH/scale_comparison.png', 
                dpi=300, bbox_inches='tight')
    print("âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: scale_comparison.png")
    plt.show()


def simulate_training_progress():
    """æ¨¡æ‹Ÿå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹"""
    
    print("=" * 70)
    print("æ¨¡æ‹Ÿæ¸è¿›å¼å“ˆå¸Œå­¦ä¹ è®­ç»ƒè¿‡ç¨‹")
    print("=" * 70)
    
    total_epochs = 100
    scale_min = 1.0
    scale_max = 10.0
    
    # æ¨¡æ‹Ÿæ•°æ®
    torch.manual_seed(42)
    x = torch.randn(1000) * 0.5
    
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  æ€»epochæ•°: {total_epochs}")
    print(f"  ScaleèŒƒå›´: {scale_min} â†’ {scale_max}")
    print(f"  æ ·æœ¬æ•°é‡: {len(x)}")
    print()
    
    # è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
    epochs_to_check = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]
    
    print("è®­ç»ƒè¿›åº¦ | Scale | æ¥è¿‘äºŒå€¼åŒ–æ¯”ä¾‹ | å¹³å‡ç»å¯¹å€¼")
    print("-" * 70)
    
    for epoch in epochs_to_check:
        # è®¡ç®—å½“å‰scale
        progress = epoch / max(total_epochs - 1, 1)
        current_scale = scale_min + progress * (scale_max - scale_min)
        
        # åº”ç”¨å˜æ¢
        y = torch.tanh(current_scale * x)
        
        # ç»Ÿè®¡ä¿¡æ¯
        close_to_binary = ((y < -0.8) | (y > 0.8)).sum().item() / len(y) * 100
        mean_abs = torch.abs(y).mean().item()
        
        print(f"  {epoch:3d}/{total_epochs}  | {current_scale:5.2f} |    {close_to_binary:5.1f}%     |   {mean_abs:.3f}")
    
    print("=" * 70)
    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("  - è®­ç»ƒåˆæœŸï¼ˆscaleâ‰ˆ1ï¼‰ï¼šå“ˆå¸Œç åˆ†å¸ƒè¾ƒåˆ†æ•£ï¼Œä¾¿äºä¼˜åŒ–")
    print("  - è®­ç»ƒä¸­æœŸï¼ˆscaleâ‰ˆ5ï¼‰ï¼šå“ˆå¸Œç é€æ¸å‘Â±1é æ‹¢")
    print("  - è®­ç»ƒæœ«æœŸï¼ˆscaleâ‰ˆ10ï¼‰ï¼šå¤§éƒ¨åˆ†å“ˆå¸Œç æ¥è¿‘Â±1ï¼Œå®ç°äºŒå€¼åŒ–")
    print()


if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("æ¸è¿›å¼å“ˆå¸Œå­¦ä¹  (Progressive Hash Learning) æµ‹è¯•")
    print("=" * 70)
    print()
    print("è¿™ä¸ªæµ‹è¯•æ¼”ç¤ºäº†HashNetçš„æ ¸å¿ƒæ€æƒ³ï¼š")
    print("  1. è®­ç»ƒåˆæœŸä½¿ç”¨å°çš„scaleå€¼ï¼Œè®©å“ˆå¸Œç ä¿æŒè¿ç»­æ€§")
    print("  2. è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¢å¤§scaleï¼Œæ¨åŠ¨å“ˆå¸Œç å‘Â±1é æ‹¢")
    print("  3. è®­ç»ƒæœ«æœŸä½¿ç”¨å¤§çš„scaleå€¼ï¼Œå®ç°æ¥è¿‘äºŒå€¼åŒ–çš„æ•ˆæœ")
    print()
    
    # è¿è¡Œæµ‹è¯•
    simulate_training_progress()
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_progressive_hash()
    compare_hash_distributions()
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
