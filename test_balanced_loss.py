"""
æµ‹è¯•åŠ æƒå¹³è¡¡æŸå¤±å‡½æ•°

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†ContrastiveLossBalancedç›¸æ¯”åŸå§‹ContrastiveLossçš„æ”¹è¿›ã€‚
å¯ä»¥é€šè¿‡ç®€å•çš„ç¤ºä¾‹çœ‹åˆ°åŠ æƒå¹³è¡¡çš„æ•ˆæœã€‚
"""

import torch
import torch.nn.functional as F
from metric import ContrastiveLoss, ContrastiveLossBalanced

def test_loss_comparison():
    """å¯¹æ¯”åŸå§‹æŸå¤±å’ŒåŠ æƒå¹³è¡¡æŸå¤±"""
    
    print("=" * 70)
    print("å¯¹æ¯”åŸå§‹ContrastiveLosså’ŒåŠ æƒå¹³è¡¡çš„ContrastiveLossBalanced")
    print("=" * 70)
    
    # è®¾ç½®å‚æ•°
    batch_size = 8
    dim = 512
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºä¸¤ä¸ªæŸå¤±å‡½æ•°
    loss_original = ContrastiveLoss(batch_size=batch_size, device=device)
    loss_balanced = ContrastiveLossBalanced(batch_size=batch_size, device=device)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    # æ¨¡æ‹Ÿåœºæ™¯ï¼šå›¾åƒå’Œæ–‡æœ¬åµŒå…¥
    torch.manual_seed(42)
    emb_i = torch.randn(batch_size, dim).to(device)  # å›¾åƒåµŒå…¥
    emb_j = torch.randn(batch_size, dim).to(device)  # æ–‡æœ¬åµŒå…¥
    
    # è®©é…å¯¹çš„å›¾æ–‡æ›´ç›¸ä¼¼ï¼ˆæ¨¡æ‹ŸçœŸå®æƒ…å†µï¼‰
    for i in range(batch_size):
        # è®©ç¬¬iä¸ªå›¾åƒå’Œç¬¬iä¸ªæ–‡æœ¬æ›´ç›¸ä¼¼
        emb_j[i] = emb_j[i] + 0.5 * emb_i[i]
    
    # è®¡ç®—ä¸¤ç§æŸå¤±
    with torch.no_grad():
        loss1 = loss_original(emb_i, emb_j)
        loss2 = loss_balanced(emb_i, emb_j)
    
    print(f"\nğŸ“Š æŸå¤±å€¼å¯¹æ¯”ï¼š")
    print(f"åŸå§‹ ContrastiveLoss:        {loss1.item():.4f}")
    print(f"åŠ æƒ ContrastiveLossBalanced: {loss2.item():.4f}")
    print(f"å·®å¼‚:                        {abs(loss1.item() - loss2.item()):.4f}")
    
    # åˆ†ææƒé‡å·®å¼‚
    print(f"\nğŸ’¡ åŠ æƒæœºåˆ¶åˆ†æï¼š")
    print(f"Batch size: {batch_size}")
    print(f"æ­£æ ·æœ¬å¯¹æ•°é‡: {batch_size} (å¯¹è§’çº¿ï¼Œé…å¯¹çš„å›¾æ–‡)")
    print(f"è´Ÿæ ·æœ¬å¯¹æ•°é‡: {batch_size * (batch_size - 1)} (éå¯¹è§’çº¿ï¼Œä¸é…å¯¹çš„å›¾æ–‡)")
    
    S1 = batch_size
    S0 = batch_size * (batch_size - 1)
    S = S1 + S0
    
    positive_weight = S / S1
    negative_weight = S / S0
    
    print(f"\nâš–ï¸  æƒé‡æ¯”ä¾‹ï¼š")
    print(f"æ­£æ ·æœ¬æƒé‡: {positive_weight:.2f}x")
    print(f"è´Ÿæ ·æœ¬æƒé‡: {negative_weight:.2f}x")
    print(f"æƒé‡æ¯”: {positive_weight / negative_weight:.2f}:1")
    
    print(f"\nâœ… è¿™æ„å‘³ç€æ¨¡å‹ä¼šæ›´åŠ å…³æ³¨'ä»€ä¹ˆæ˜¯ç›¸ä¼¼çš„'ï¼ˆæ­£æ ·æœ¬ï¼‰")
    print(f"   è€Œä¸æ˜¯ç®€å•è®°ä½'å¤§éƒ¨åˆ†éƒ½ä¸ç›¸ä¼¼'ï¼ˆè´Ÿæ ·æœ¬ï¼‰")
    
    return loss1, loss2


def visualize_weight_effect():
    """å¯è§†åŒ–ä¸åŒbatch sizeä¸‹çš„æƒé‡æ•ˆæœ"""
    
    print("\n" + "=" * 70)
    print("ä¸åŒBatch Sizeä¸‹çš„æƒé‡æ•ˆæœ")
    print("=" * 70)
    
    batch_sizes = [4, 8, 16, 32, 64, 128]
    
    print(f"\n{'Batch Size':<12} {'æ­£æ ·æœ¬æ•°':<10} {'è´Ÿæ ·æœ¬æ•°':<12} {'æ­£æƒé‡':<10} {'è´Ÿæƒé‡':<10} {'æƒé‡æ¯”':<10}")
    print("-" * 70)
    
    for bs in batch_sizes:
        S1 = bs
        S0 = bs * (bs - 1)
        S = S1 + S0
        
        pos_weight = S / S1
        neg_weight = S / S0
        weight_ratio = pos_weight / neg_weight
        
        print(f"{bs:<12} {S1:<10} {S0:<12} {pos_weight:<10.2f} {neg_weight:<10.2f} {weight_ratio:<10.2f}")
    
    print("\nğŸ’¡ è§‚å¯Ÿï¼š")
    print("   - Batch sizeè¶Šå¤§ï¼Œæ­£è´Ÿæ ·æœ¬çš„ä¸å¹³è¡¡è¶Šä¸¥é‡")
    print("   - æƒé‡æ¯”ä¹Ÿéšä¹‹å¢å¤§ï¼Œæ›´å¼ºè°ƒæ­£æ ·æœ¬çš„é‡è¦æ€§")
    print("   - è¿™æ­£æ˜¯åŠ æƒå¹³è¡¡ç­–ç•¥çš„ä»·å€¼æ‰€åœ¨ï¼")


def test_gradient_comparison():
    """å¯¹æ¯”ä¸¤ç§æŸå¤±å‡½æ•°çš„æ¢¯åº¦"""
    
    print("\n" + "=" * 70)
    print("æ¢¯åº¦å¯¹æ¯”æµ‹è¯•ï¼ˆæ£€æŸ¥å“ªç§æŸå¤±å‡½æ•°å­¦ä¹ æ•ˆç‡æ›´é«˜ï¼‰")
    print("=" * 70)
    
    batch_size = 8
    dim = 512
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    # åˆ›å»ºå¯è®­ç»ƒçš„åµŒå…¥
    emb_i = torch.randn(batch_size, dim, requires_grad=True, device=device)
    emb_j = torch.randn(batch_size, dim, requires_grad=True, device=device)
    
    # æµ‹è¯•åŸå§‹æŸå¤±
    loss_fn1 = ContrastiveLoss(batch_size=batch_size, device=device)
    loss1 = loss_fn1(emb_i, emb_j)
    loss1.backward()
    grad_norm1 = emb_i.grad.norm().item()
    
    # é‡ç½®æ¢¯åº¦
    emb_i.grad = None
    emb_j.grad = None
    
    # æµ‹è¯•åŠ æƒæŸå¤±
    loss_fn2 = ContrastiveLossBalanced(batch_size=batch_size, device=device)
    loss2 = loss_fn2(emb_i, emb_j)
    loss2.backward()
    grad_norm2 = emb_i.grad.norm().item()
    
    print(f"\nğŸ“ˆ æ¢¯åº¦èŒƒæ•°å¯¹æ¯”ï¼š")
    print(f"åŸå§‹æŸå¤±æ¢¯åº¦èŒƒæ•°: {grad_norm1:.4f}")
    print(f"åŠ æƒæŸå¤±æ¢¯åº¦èŒƒæ•°: {grad_norm2:.4f}")
    print(f"å·®å¼‚æ¯”ä¾‹: {grad_norm2/grad_norm1:.2f}x")
    
    if grad_norm2 > grad_norm1:
        print(f"\nâœ… åŠ æƒæŸå¤±äº§ç”Ÿäº†æ›´å¤§çš„æ¢¯åº¦ï¼Œè¿™é€šå¸¸æ„å‘³ç€ï¼š")
        print(f"   - å¯¹æ­£æ ·æœ¬çš„å­¦ä¹ ä¿¡å·æ›´å¼º")
        print(f"   - æ¨¡å‹ä¼šæ›´å¿«åœ°å­¦ä¹ 'ä»€ä¹ˆæ˜¯ç›¸ä¼¼çš„'")
    

if __name__ == "__main__":
    print("\n" + "ğŸ”¬" * 35)
    print("HashNetåŠ æƒå¹³è¡¡ç­–ç•¥ - æµ‹è¯•è„šæœ¬")
    print("ğŸ”¬" * 35)
    
    # æµ‹è¯•1ï¼šåŸºæœ¬æŸå¤±å¯¹æ¯”
    test_loss_comparison()
    
    # æµ‹è¯•2ï¼šå¯è§†åŒ–æƒé‡æ•ˆæœ
    visualize_weight_effect()
    
    # æµ‹è¯•3ï¼šæ¢¯åº¦å¯¹æ¯”
    test_gradient_comparison()
    
    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)
    print("\nğŸ’¡ æ€»ç»“ï¼š")
    print("   1. ContrastiveLossBalancedé€šè¿‡åŠ æƒå¹³è¡¡æ­£è´Ÿæ ·æœ¬")
    print("   2. æ­£æ ·æœ¬æƒé‡ >> è´Ÿæ ·æœ¬æƒé‡ï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨ç›¸ä¼¼æ€§å­¦ä¹ ")
    print("   3. è¿™æ˜¯HashNetçš„æ ¸å¿ƒæ€æƒ³ï¼Œèƒ½æœ‰æ•ˆç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
    print("\nğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼æœŸå¾…æ€§èƒ½æå‡ï¼\n")
